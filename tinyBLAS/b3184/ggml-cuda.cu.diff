[1mdiff --git a/ggml-cuda-rollup.cu b/home/benuix/ggml-cuda.cu[m
[1mindex cd8b2f08..42e5fada 100644[m
[1m--- a/ggml-cuda-rollup.cu[m
[1m+++ b/home/benuix/ggml-cuda.cu[m
[36m@@ -58,12 +58,44 @@[m

#if defined(GGML_USE_HIPBLAS)[m
#include <hip/hip_runtime.h>[m
[31m#include <hipblas/hipblas.h>[m
#include <hip/hip_fp16.h>[m
[32m#include <hip/hip_bfloat16.h>[m

[32m#ifdef GGML_USE_TINYBLAS[m
[32m#include "tinyblas.cu"[m
[32m#define __nv_bfloat16 hip_bfloat16[m
[32m#define CUBLAS_COMPUTE_16F TINYBLAS_COMPUTE_16F[m
[32m#define CUBLAS_COMPUTE_32F TINYBLAS_COMPUTE_32F[m
[32m#define CUBLAS_COMPUTE_32F_FAST_16F TINYBLAS_COMPUTE_32F[m
[32m#define CUBLAS_GEMM_DEFAULT TINYBLAS_GEMM_DEFAULT[m
[32m#define CUBLAS_GEMM_DEFAULT_TENSOR_OP TINYBLAS_GEMM_DEFAULT[m
[32m#define CUBLAS_OP_N TINYBLAS_OP_N[m
[32m#define CUBLAS_OP_T TINYBLAS_OP_T[m
[32m#define CUBLAS_STATUS_SUCCESS TINYBLAS_STATUS_SUCCESS[m
[32m#define CUBLAS_TF32_TENSOR_OP_MATH 0[m
[32m#define CUDA_R_16F TINYBLAS_R_16F[m
[32m#define CUDA_R_32F TINYBLAS_R_32F[m
[32m#define cublasComputeType_t tinyblasComputeType_t[m
[32m#define cublasCreate tinyblasCreate[m
[32m#define cublasDestroy tinyblasDestroy[m
[32m#define cublasGemmEx tinyblasGemmEx[m
[32m#define cublasGemmBatchedEx tinyblasGemmBatchedEx[m
[32m#define cublasGemmStridedBatchedEx tinyblasGemmStridedBatchedEx[m
[32m#define cublasHandle_t tinyblasHandle_t[m
[32m#define cublasSetMathMode(handle, mode) CUBLAS_STATUS_SUCCESS[m
[32m#define cublasSetStream tinyblasSetStream[m
[32m#define cublasSgemm tinyblasSgemm[m
[32m#define cublasStatus_t tinyblasStatus_t[m
[32m#define cudaDataType_t tinyblasDataType_t[m
[32m#define cublasGetStatusString tinyblasGetStatusString[m

[32m#else[m
[32m#include <hipblas/hipblas.h>[m
#ifdef __HIP_PLATFORM_AMD__[m
// for rocblas_initialize()[m
#include "rocblas/rocblas.h"[m
#endif // __HIP_PLATFORM_AMD__[m
[32m#define __nv_bfloat16 hip_bfloat16[m
#define CUBLAS_COMPUTE_16F HIPBLAS_R_16F[m
#define CUBLAS_COMPUTE_32F HIPBLAS_R_32F[m
#define CUBLAS_COMPUTE_32F_FAST_16F HIPBLAS_R_32F[m
[36m@@ -75,7 +107,6 @@[m
#define CUBLAS_TF32_TENSOR_OP_MATH 0[m
#define CUDA_R_16F  HIPBLAS_R_16F[m
#define CUDA_R_32F  HIPBLAS_R_32F[m
[31m#define __shfl_xor_sync(mask, var, laneMask, width) __shfl_xor(var, laneMask, width)[m
#define cublasComputeType_t hipblasDatatype_t //deprecated, new hipblasComputeType_t not in 5.6[m
#define cublasCreate hipblasCreate[m
#define cublasDestroy hipblasDestroy[m
[36m@@ -88,6 +119,18 @@[m
#define cublasSgemm hipblasSgemm[m
#define cublasStatus_t hipblasStatus_t[m
#define cudaDataType_t hipblasDatatype_t //deprecated, new hipblasDatatype not in 5.6[m
[32m#define CUBLAS_STATUS_SUCCESS HIPBLAS_STATUS_SUCCESS[m
[32m#define CUBLAS_STATUS_NOT_INITIALIZED HIPBLAS_STATUS_NOT_INITIALIZED[m
[32m#define CUBLAS_STATUS_ALLOC_FAILED HIPBLAS_STATUS_ALLOC_FAILED[m
[32m#define CUBLAS_STATUS_INVALID_VALUE HIPBLAS_STATUS_INVALID_VALUE[m
[32m#define CUBLAS_STATUS_ARCH_MISMATCH HIPBLAS_STATUS_ARCH_MISMATCH[m
[32m#define CUBLAS_STATUS_MAPPING_ERROR HIPBLAS_STATUS_MAPPING_ERROR[m
[32m#define CUBLAS_STATUS_EXECUTION_FAILED HIPBLAS_STATUS_EXECUTION_FAILED[m
[32m#define CUBLAS_STATUS_INTERNAL_ERROR HIPBLAS_STATUS_INTERNAL_ERROR[m
[32m#define CUBLAS_STATUS_NOT_SUPPORTED HIPBLAS_STATUS_NOT_SUPPORTED[m
[32m#endif[m

[32m#define __shfl_xor_sync(mask, var, laneMask, width) __shfl_xor(var, laneMask, width)[m
#define cudaDeviceCanAccessPeer hipDeviceCanAccessPeer[m
#define cudaDeviceDisablePeerAccess hipDeviceDisablePeerAccess[m
#define cudaDeviceEnablePeerAccess hipDeviceEnablePeerAccess[m
[36m@@ -139,20 +182,43 @@[m
#define cudaStream_t hipStream_t[m
#define cudaSuccess hipSuccess[m
#define __trap abort[m
[32m#elif defined(GGML_USE_TINYBLAS)[m
[32m#include <cuda_runtime.h>[m
[32m#include <cuda.h>[m
[32m#include <cuda_fp16.h>[m
[32m#include <cuda_bf16.h>[m
[32m#include "tinyblas.cu"[m

[32m#define CUBLAS_COMPUTE_16F TINYBLAS_COMPUTE_16F[m
[32m#define CUBLAS_COMPUTE_32F TINYBLAS_COMPUTE_32F[m
[32m#define CUBLAS_OP_N TINYBLAS_OP_N[m
[32m#define CUBLAS_OP_T TINYBLAS_OP_T[m
[32m#define CUDA_R_16F TINYBLAS_R_16F[m
[32m#define CUDA_R_32F TINYBLAS_R_32F[m
[32m#define CUBLAS_GEMM_DEFAULT TINYBLAS_GEMM_DEFAULT[m
[32m#define CUBLAS_GEMM_DEFAULT_TENSOR_OP TINYBLAS_GEMM_DEFAULT[m
#define CUBLAS_STATUS_SUCCESS [31mHIPBLAS_STATUS_SUCCESS[m[32mTINYBLAS_STATUS_SUCCESS[m
#define [31mCUBLAS_STATUS_NOT_INITIALIZED HIPBLAS_STATUS_NOT_INITIALIZED[m[32mcublasGemmAlgo_t tinyblasGemmAlgo_t[m
#define [31mCUBLAS_STATUS_ALLOC_FAILED HIPBLAS_STATUS_ALLOC_FAILED[m[32mcublasOperation_t tinyblasOperation_t[m
#define [31mCUBLAS_STATUS_INVALID_VALUE HIPBLAS_STATUS_INVALID_VALUE[m[32mcublasComputeType_t tinyblasComputeType_t[m
#define [31mCUBLAS_STATUS_ARCH_MISMATCH HIPBLAS_STATUS_ARCH_MISMATCH[m[32mcublasHandle_t tinyblasHandle_t[m
#define [31mCUBLAS_STATUS_MAPPING_ERROR HIPBLAS_STATUS_MAPPING_ERROR[m[32mcublasStatus_t tinyblasStatus_t[m
#define [31mCUBLAS_STATUS_EXECUTION_FAILED HIPBLAS_STATUS_EXECUTION_FAILED[m[32mcublasSgemm tinyblasSgemm[m
#define [31mCUBLAS_STATUS_INTERNAL_ERROR HIPBLAS_STATUS_INTERNAL_ERROR[m[32mcublasGemmEx tinyblasGemmEx[m
#define [31mCUBLAS_STATUS_NOT_SUPPORTED HIPBLAS_STATUS_NOT_SUPPORTED[m[32mcublasCreate tinyblasCreate[m
[32m#define cublasDestroy tinyblasDestroy[m
[32m#define cublasSetStream tinyblasSetStream[m
[32m#define cublasGemmBatchedEx tinyblasGemmBatchedEx[m
[32m#define cublasGemmStridedBatchedEx tinyblasGemmStridedBatchedEx[m
[32m#define cublasGetStatusString tinyblasGetStatusString[m
[32m#define cudaDataType_t tinyblasDataType_t[m
[32m#define cublasSetMathMode(handle, mode) CUBLAS_STATUS_SUCCESS[m
#else[m
#include <cuda_runtime.h>[m
#include <cuda.h>[m
#include <cublas_v2.h>[m
#include <cuda_fp16.h>[m
[32m#include <cuda_bf16.h>[m

#if CUDART_VERSION < 11020[m
#define CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED[m
[36m@@ -164,6 +230,15 @@[m

#endif // defined(GGML_USE_HIPBLAS)[m

[32m#include "ggml-backend-impl.h"[m

[32m#ifdef GGML_USE_TINYBLAS[m
[32m#define BLAS_NAME "tinyBLAS"[m
[32m#else[m
[32m#define BLAS_NAME GGML_CUBLAS_NAME[m
[32m#endif[m


#define STRINGIZE_IMPL(...) #__VA_ARGS__[m
#define STRINGIZE(...) STRINGIZE_IMPL(__VA_ARGS__)[m

[36m@@ -187,7 +262,11 @@[m
// -  7B quantum model: +100-200 MB[m
// - 13B quantum model: +200-400 MB[m
//[m
[31m//#define GGML_CUDA_FORCE_MMQ[m[32m// [jart] https://github.com/Mozilla-Ocho/llamafile/issues/403#issuecomment-2103687594[m
[32m#ifdef GGML_USE_TINYBLAS[m
[32m#define GGML_CUDA_FORCE_MMQ// [jart] want this[m
[32m#endif[m


// TODO: improve this to be correct for more hardware[m
//       for example, currently fails for GeForce GTX 1660 which is TURING arch (> VOLTA) but does not have tensor cores[m
[36m@@ -219,7 +298,7 @@[m [mvoid ggml_cuda_error(const char * stmt, const char * func, const char * file, in[m

#define CUDA_CHECK(err) CUDA_CHECK_GEN(err, cudaSuccess, cudaGetErrorString)[m

#if CUDART_VERSION >= [31m12000[m[32m12000|| defined(GGML_USE_TINYBLAS)[m
    static const char * cublas_get_error_str(const cublasStatus_t err) {[m
        return cublasGetStatusString(err);[m
    }[m
[36m@@ -13152,8 +13231,10 @@[m [mstatic ggml_cuda_device_info ggml_cuda_init() {[m
#ifdef __HIP_PLATFORM_AMD__[m
    // Workaround for a rocBLAS bug when using multiple graphics cards:[m
    // https://github.com/ROCmSoftwarePlatform/rocBLAS/issues/1346[m
[32m#ifndef GGML_USE_TINYBLAS[m
    rocblas_initialize();[m
    CUDA_CHECK(cudaDeviceSynchronize());[m
[32m#endif[m
#endif[m

    ggml_cuda_device_info info = {};[m
