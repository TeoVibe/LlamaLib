name: build

on:
  push:

jobs:
  server:
    runs-on: ubuntu-22.04

    strategy:
      matrix:
        include:
          - avx: avx2
            avx_defines: ''

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Checkout llama.cpp
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          path: repo

      - name: Get number of CPU cores
        uses: SimenB/github-actions-cpu-cores@v2
        id: cpu-cores

      - name: Setup Alpine Linux
        uses: jirutka/setup-alpine@v1
        with:
          packages: build-base cmake ccache git openssl-dev openssl-libs-static

      - name: CMake
        run: |
          cmake repo -B build -DCMAKE_BUILD_TYPE=Release -DCMAKE_EXE_LINKER_FLAGS="-static" -DGGML_NATIVE=OFF -DGGML_STATIC=ON -DGGML_OPENMP=OFF -DLLAMA_BUILD_SERVER=ON -DBUILD_SHARED_LIBS=OFF ${{ matrix.ssl_defines }} ${{ matrix.avx_defines }}
          cmake --build build --config Release --target llama-server -j ${{ steps.cpu-cores.outputs.count }}
          strip build/bin/llama-server
        shell: alpine.sh {0}
