name: Build library

on:
  push:
    paths:
      - .github/workflows/test.yaml

env:
  LLAMACPP_VERSION: b3580
  CMAKE_COMMON_JOBS: '-DGGML_STATIC=ON -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF -DBUILD_UNDREAMAI_SERVER=ON -DBUILD_SHARED_LIBS=OFF'
  CMAKE_COMMON_DIR: -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs

jobs:
  windows-cuda-build:
    runs-on: windows-2019

    env:
      CMAKE_COMMON: '-DGGML_NATIVE=OFF'
    
    strategy:
      matrix:
        include:
          - build: 'cuda-cu12.2.0'
            defines: '-DGGML_CUDA=ON'

    steps:
      - name: Clone
        id: checkout
        uses: actions/checkout@v4

      - name: Get Variables
        run: |
          $pattern = "\d+\.\d+\.\d+"
          $CUDA = [regex]::Match("${{ matrix.build }}", $pattern).Value
          Add-Content $env:GITHUB_ENV "CUDA=$CUDA"

      - name: Clone llama.cpp
        run: |
          git clone https://github.com/ggerganov/llama.cpp llama.cpp
          cd llama.cpp
          git checkout ${{ env.LLAMACPP_VERSION }}
          (Get-Content examples/server/server.cpp ) -replace 'utils.hpp', 'utils_callback.hpp' | Set-Content examples/server/server.cpp
          (Get-Content examples/server/server.cpp ) -replace 'main\(int argc', 'main_server(int argc' | Set-Content examples/server/server.cpp

      - name: Create missing files
        run: |
          cd llama.cpp
          Get-ChildItem examples/server/public/* | ForEach-Object {
            $f = $_.FullName
            $output = $f -replace "public\\", ""
            echo "cmake -DINPUT=$f -DOUTPUT=$output.hpp -P scripts/xxd.cmake"
            cmake -DINPUT="$f" -DOUTPUT="$output.hpp" -P "scripts/xxd.cmake"
          }
          
      - name: Prepare CUDA
        run: |
          cp tinyBLAS/${env:LLAMACPP_VERSION}/* llama.cpp\ggml\src\
          rm -r llama.cpp/ggml/src/ggml-cuda
          sed -i 's:GGML_USE_CUDA:GGML_USE_CUDA GGML_USE_TINYBLAS NDEBUG GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS:g' llama.cpp/ggml/src/CMakeLists.txt

      - uses: Jimver/cuda-toolkit@v0.2.15
        id: cuda-toolkit
        with:
          cuda: ${{ env.CUDA }}
          method: 'network'
          sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'

      - name: Get number of CPU cores
        uses: SimenB/github-actions-cpu-cores@v2
        id: cpu-cores

      - name: Build
        id: cmake_build
        run: |
          (Get-Content CMakeLists.txt ) -replace 'LIBRARY undreamai', 'LIBRARY undreamai_windows-${{ matrix.build }}' | Set-Content CMakeLists.txt
          mkdir build
          cd build
          cmake .. ${{ matrix.defines }} ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}
          cmake --build . --config Release -j ${{ steps.cpu-cores.outputs.count }}

      - name: Pack artifacts
        id: pack_artifacts
        run: |
          del build/libs/Release/undreamai_test.*
          mkdir artifacts
          ls -R build
          curl.exe -o artifacts\llamafile.LICENSE.txt -L "https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE"
          Copy-Item .\llama.cpp\LICENSE .\artifacts\llama.cpp.LICENSE.txt
          Copy-Item .\build\Release\* .\artifacts\
          Copy-Item .\build\libs\Release\*dll .\artifacts\
          $serverPath = ".\build\libs\Release\undreamai_server.exe"
          if (Test-Path $serverPath) {
              Copy-Item $serverPath -Destination ".\artifacts\"
          }
          cd artifacts
          7z a ../undreamai-test-llamacpp-windows-${{ matrix.build }}.zip *

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          path: undreamai-test-llamacpp-windows-${{ matrix.build }}.zip
          name: undreamai-test-llamacpp-windows-${{ matrix.build }}.zip
